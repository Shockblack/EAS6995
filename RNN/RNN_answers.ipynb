{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2\n",
    "\n",
    "### 2.1\n",
    "- $W_{xh}$ represents the weights from the input data into the hidden layer. $W_{hh}$ represents the weights between the $h_{t-1}$ to $h_{t}$ layers. And $W_{hy}$ is the weights between the hidden layer $h$ and output layer $y$.\n",
    "- We use `tanh` because we seek non-linearity in the learning. We use `tanh` specifically because it is zero centered, with a range of [-1,1].\n",
    "- The hidden state is initialized to a zero matrix so that they can be inputed into the $h_{t-1}$ layer and eventually learn.\n",
    "\n",
    "### 2.2\n",
    "- The loss function being used is cross-entropy loss. This is because we want to predict the next character, and penalize the network for incorrectly predicting the next character.\n",
    "- The gradient is backpropogated through each time step, where the weight of the $h_t$ is updated based on the prior time step. The weights for each time step are summed to the update.\n",
    "- The gradient clipping is to stop the gradient from exploding, since you are constantly adding values to the weight updates. \n",
    "\n",
    "### 2.3\n",
    "- Words are generated by taking the current state of weights and an initial first letter. This is encoded as an index that correspondeds to a specific letter/character. The model is calculated, and an output distribution of probabilites emerges. The next letter in the sequence is drawn from this probability distribution, rather than using only the most probable letter. The cycle then repeats.\n",
    "- Uh... kinda explained above. It takes the range of possible indexes associated with characters and assigns them a probability. The function `np.random.choice` then takes a random character of that sequence following the probability distribution calculated by the network.\n",
    "- If you tightened/narrowed the probability distrubtion, you would get less varied outputs and the network would probably stop learning. On the other hand, if the distribution is too wide your network will struggle to produce anything that is better than nonsense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
